{
	"jobConfig": {
		"name": "automate_etl_support_tickets",
		"description": "",
		"role": "arn:aws:iam::127996279779:role/glue_support_tickets",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 2,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "automate_etl_support_tickets.py",
		"scriptLocation": "s3://aws-glue-assets-127996279779-ap-south-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-09-17T21:05:09.585Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-127996279779-ap-south-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-127996279779-ap-south-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.gluetypes import *\r\nfrom awsgluedq.transforms import EvaluateDataQuality\r\nfrom awsglue import DynamicFrame\r\nimport re\r\n\r\ndef _find_null_fields(ctx, schema, path, output, nullStringSet, nullIntegerSet, frame):\r\n    if isinstance(schema, StructType):\r\n        for field in schema:\r\n            new_path = path + \".\" if path != \"\" else path\r\n            output = _find_null_fields(ctx, field.dataType, new_path + field.name, output, nullStringSet, nullIntegerSet, frame)\r\n    elif isinstance(schema, ArrayType):\r\n        if isinstance(schema.elementType, StructType):\r\n            output = _find_null_fields(ctx, schema.elementType, path, output, nullStringSet, nullIntegerSet, frame)\r\n    elif isinstance(schema, NullType):\r\n        output.append(path)\r\n    else:\r\n        x, distinct_set = frame.toDF(), set()\r\n        for i in x.select(path).distinct().collect():\r\n            distinct_ = i[path.split('.')[-1]]\r\n            if isinstance(distinct_, list):\r\n                distinct_set |= set([item.strip() if isinstance(item, str) else item for item in distinct_])\r\n            elif isinstance(distinct_, str) :\r\n                distinct_set.add(distinct_.strip())\r\n            else:\r\n                distinct_set.add(distinct_)\r\n        if isinstance(schema, StringType):\r\n            if distinct_set.issubset(nullStringSet):\r\n                output.append(path)\r\n        elif isinstance(schema, IntegerType) or isinstance(schema, LongType) or isinstance(schema, DoubleType):\r\n            if distinct_set.issubset(nullIntegerSet):\r\n                output.append(path)\r\n    return output\r\n\r\ndef drop_nulls(glueContext, frame, nullStringSet, nullIntegerSet, transformation_ctx) -> DynamicFrame:\r\n    nullColumns = _find_null_fields(frame.glue_ctx, frame.schema(), \"\", [], nullStringSet, nullIntegerSet, frame)\r\n    return DropFields.apply(frame=frame, paths=nullColumns, transformation_ctx=transformation_ctx)\r\n\r\ndef sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:\r\n    for alias, frame in mapping.items():\r\n        frame.toDF().createOrReplaceTempView(alias)\r\n    result = spark.sql(query)\r\n    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\r\n\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME','input_file_path'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Default ruleset (simplified to a single line to prevent syntax errors)\r\nDEFAULT_DATA_QUALITY_RULESET = \"Rules = [ ColumnCount > 0 ]\"\r\n\r\n# Script generated for node Amazon S3\r\nAmazonS3_node1757937600559 = glueContext.create_dynamic_frame.from_options(format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \",\", \"optimizePerformance\": False}, connection_type=\"s3\", format=\"csv\", connection_options={\"paths\": [args['input_file_path']], \"recurse\": True}, transformation_ctx=\"AmazonS3_node1757937600559\")\r\n\r\n# Script generated for node Change Schema\r\nChangeSchema_node1757937912813 = ApplyMapping.apply(frame=AmazonS3_node1757937600559, mappings=[(\"ticket_id\", \"string\", \"ticket_id\", \"string\"), (\"created_at\", \"string\", \"created_at\", \"timestamp\"), (\"resolved_at\", \"string\", \"resolved_at\", \"timestamp\"), (\"agent\", \"string\", \"agent\", \"string\"), (\"priority\", \"string\", \"priority\", \"string\"), (\"num_interactions\", \"string\", \"num_interactions\", \"int\"), (\"issuecat\", \"string\", \"issuecat\", \"string\"), (\"channel\", \"string\", \"channel\", \"string\"), (\"status\", \"string\", \"status\", \"string\"), (\"agent_feedback\", \"string\", \"agent_feedback\", \"string\")], transformation_ctx=\"ChangeSchema_node1757937912813\")\r\n\r\n# Script generated for node Drop Null Fields\r\nDropNullFields_node1757938049339 = drop_nulls(glueContext, frame=ChangeSchema_node1757937912813, nullStringSet={\"\"}, nullIntegerSet={}, transformation_ctx=\"DropNullFields_node1757938049339\")\r\n\r\n# Script generated for node Rename Field- Issue Category\r\nRenameFieldIssueCategory_node1757938107279 = RenameField.apply(frame=DropNullFields_node1757938049339, old_name=\"issuecat\", new_name=\"issue_category\", transformation_ctx=\"RenameFieldIssueCategory_node1757938107279\")\r\n\r\n# Script generated for node Filter-num_interactions\r\nFilternum_interactions_node1757938209331 = Filter.apply(frame=RenameFieldIssueCategory_node1757938107279, f=lambda row: (row[\"num_interactions\"] >= 0), transformation_ctx=\"Filternum_interactions_node1757938209331\")\r\n\r\n# Script generated for node SQL Query - Priority\r\nSqlQuery_Corrected = '''\r\nSELECT\r\n    ticket_id,\r\n    created_at,\r\n    resolved_at,\r\n    agent,\r\n    num_interactions,\r\n    issue_category,\r\n    channel,\r\n    status,\r\n    CASE \r\n        WHEN priority = \"Lw\" THEN \"Low\"\r\n        WHEN priority = \"Medum\" THEN \"Medium\"\r\n        WHEN priority = \"Hgh\" THEN \"High\"\r\n        ELSE priority\r\n    END AS priority\r\nFROM myDataSource\r\n'''\r\nSQLQueryPriority_node1757938328239 = sparkSqlQuery(glueContext, query = SqlQuery_Corrected, mapping = {\"myDataSource\":Filternum_interactions_node1757938209331}, transformation_ctx = \"SQLQueryPriority_node1757938328239\")\r\n\r\n# Script generated for node Select Fields\r\nSelectFields_node1758120293070 = SelectFields.apply(frame=SQLQueryPriority_node1757938328239, paths=[\"ticket_id\", \"created_at\", \"resolved_at\", \"agent\", \"priority\", \"num_interactions\", \"issue_category\", \"channel\", \"status\"], transformation_ctx=\"SelectFields_node1758120293070\")\r\n\r\n# Script generated for node Amazon S3\r\nEvaluateDataQuality().process_rows(frame=SelectFields_node1758120293070, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1757939356118\", \"enableDataQualityResultsPublishing\": True}, additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"})\r\nif (SelectFields_node1758120293070.count() >= 1):\r\n    SelectFields_node1758120293070 = SelectFields_node1758120293070.coalesce(1)\r\nAmazonS3_node1757939754937 = glueContext.write_dynamic_frame.from_options(frame=SelectFields_node1758120293070, connection_type=\"s3\", format=\"glueparquet\", connection_options={\"path\": \"s3://careplus-data-5173/Support-tickets/processed/\", \"partitionKeys\": []}, format_options={\"compression\": \"snappy\"}, transformation_ctx=\"AmazonS3_node1757939754937\")\r\n\r\njob.commit()"
}