{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32142bc9-3a9e-42bb-9e83-376f097b8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.gluetypes import *\n",
    "from awsgluedq.transforms import EvaluateDataQuality\n",
    "from awsglue import DynamicFrame\n",
    "import re\n",
    "\n",
    "def _find_null_fields(ctx, schema, path, output, nullStringSet, nullIntegerSet, frame):\n",
    "    if isinstance(schema, StructType):\n",
    "        for field in schema:\n",
    "            new_path = path + \".\" if path != \"\" else path\n",
    "            output = _find_null_fields(ctx, field.dataType, new_path + field.name, output, nullStringSet, nullIntegerSet, frame)\n",
    "    elif isinstance(schema, ArrayType):\n",
    "        if isinstance(schema.elementType, StructType):\n",
    "            output = _find_null_fields(ctx, schema.elementType, path, output, nullStringSet, nullIntegerSet, frame)\n",
    "    elif isinstance(schema, NullType):\n",
    "        output.append(path)\n",
    "    else:\n",
    "        x, distinct_set = frame.toDF(), set()\n",
    "        for i in x.select(path).distinct().collect():\n",
    "            distinct_ = i[path.split('.')[-1]]\n",
    "            if isinstance(distinct_, list):\n",
    "                distinct_set |= set([item.strip() if isinstance(item, str) else item for item in distinct_])\n",
    "            elif isinstance(distinct_, str) :\n",
    "                distinct_set.add(distinct_.strip())\n",
    "            else:\n",
    "                distinct_set.add(distinct_)\n",
    "        if isinstance(schema, StringType):\n",
    "            if distinct_set.issubset(nullStringSet):\n",
    "                output.append(path)\n",
    "        elif isinstance(schema, IntegerType) or isinstance(schema, LongType) or isinstance(schema, DoubleType):\n",
    "            if distinct_set.issubset(nullIntegerSet):\n",
    "                output.append(path)\n",
    "    return output\n",
    "\n",
    "def drop_nulls(glueContext, frame, nullStringSet, nullIntegerSet, transformation_ctx) -> DynamicFrame:\n",
    "    nullColumns = _find_null_fields(frame.glue_ctx, frame.schema(), \"\", [], nullStringSet, nullIntegerSet, frame)\n",
    "    return DropFields.apply(frame=frame, paths=nullColumns, transformation_ctx=transformation_ctx)\n",
    "\n",
    "def sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:\n",
    "    for alias, frame in mapping.items():\n",
    "        frame.toDF().createOrReplaceTempView(alias)\n",
    "    result = spark.sql(query)\n",
    "    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME','input_file_path'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Default ruleset (simplified to a single line to prevent syntax errors)\n",
    "DEFAULT_DATA_QUALITY_RULESET = \"Rules = [ ColumnCount > 0 ]\"\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "AmazonS3_node1757937600559 = glueContext.create_dynamic_frame.from_options(format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \",\", \"optimizePerformance\": False}, connection_type=\"s3\", format=\"csv\", connection_options={\"paths\": [args['input_file_path']], \"recurse\": True}, transformation_ctx=\"AmazonS3_node1757937600559\")\n",
    "\n",
    "# Script generated for node Change Schema\n",
    "ChangeSchema_node1757937912813 = ApplyMapping.apply(frame=AmazonS3_node1757937600559, mappings=[(\"ticket_id\", \"string\", \"ticket_id\", \"string\"), (\"created_at\", \"string\", \"created_at\", \"timestamp\"), (\"resolved_at\", \"string\", \"resolved_at\", \"timestamp\"), (\"agent\", \"string\", \"agent\", \"string\"), (\"priority\", \"string\", \"priority\", \"string\"), (\"num_interactions\", \"string\", \"num_interactions\", \"int\"), (\"issuecat\", \"string\", \"issuecat\", \"string\"), (\"channel\", \"string\", \"channel\", \"string\"), (\"status\", \"string\", \"status\", \"string\"), (\"agent_feedback\", \"string\", \"agent_feedback\", \"string\")], transformation_ctx=\"ChangeSchema_node1757937912813\")\n",
    "\n",
    "# Script generated for node Drop Null Fields\n",
    "DropNullFields_node1757938049339 = drop_nulls(glueContext, frame=ChangeSchema_node1757937912813, nullStringSet={\"\"}, nullIntegerSet={}, transformation_ctx=\"DropNullFields_node1757938049339\")\n",
    "\n",
    "# Script generated for node Rename Field- Issue Category\n",
    "RenameFieldIssueCategory_node1757938107279 = RenameField.apply(frame=DropNullFields_node1757938049339, old_name=\"issuecat\", new_name=\"issue_category\", transformation_ctx=\"RenameFieldIssueCategory_node1757938107279\")\n",
    "\n",
    "# Script generated for node Filter-num_interactions\n",
    "Filternum_interactions_node1757938209331 = Filter.apply(frame=RenameFieldIssueCategory_node1757938107279, f=lambda row: (row[\"num_interactions\"] >= 0), transformation_ctx=\"Filternum_interactions_node1757938209331\")\n",
    "\n",
    "# Script generated for node SQL Query - Priority\n",
    "SqlQuery_Corrected = '''\n",
    "SELECT\n",
    "    ticket_id,\n",
    "    created_at,\n",
    "    resolved_at,\n",
    "    agent,\n",
    "    num_interactions,\n",
    "    issue_category,\n",
    "    channel,\n",
    "    status,\n",
    "    CASE \n",
    "        WHEN priority = \"Lw\" THEN \"Low\"\n",
    "        WHEN priority = \"Medum\" THEN \"Medium\"\n",
    "        WHEN priority = \"Hgh\" THEN \"High\"\n",
    "        ELSE priority\n",
    "    END AS priority\n",
    "FROM myDataSource\n",
    "'''\n",
    "SQLQueryPriority_node1757938328239 = sparkSqlQuery(glueContext, query = SqlQuery_Corrected, mapping = {\"myDataSource\":Filternum_interactions_node1757938209331}, transformation_ctx = \"SQLQueryPriority_node1757938328239\")\n",
    "\n",
    "# Script generated for node Select Fields\n",
    "SelectFields_node1758120293070 = SelectFields.apply(frame=SQLQueryPriority_node1757938328239, paths=[\"ticket_id\", \"created_at\", \"resolved_at\", \"agent\", \"priority\", \"num_interactions\", \"issue_category\", \"channel\", \"status\"], transformation_ctx=\"SelectFields_node1758120293070\")\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "EvaluateDataQuality().process_rows(frame=SelectFields_node1758120293070, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1757939356118\", \"enableDataQualityResultsPublishing\": True}, additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"})\n",
    "if (SelectFields_node1758120293070.count() >= 1):\n",
    "    SelectFields_node1758120293070 = SelectFields_node1758120293070.coalesce(1)\n",
    "AmazonS3_node1757939754937 = glueContext.write_dynamic_frame.from_options(frame=SelectFields_node1758120293070, connection_type=\"s3\", format=\"glueparquet\", connection_options={\"path\": \"s3://careplus-data-5173/Support-tickets/processed/\", \"partitionKeys\": []}, format_options={\"compression\": \"snappy\"}, transformation_ctx=\"AmazonS3_node1757939754937\")\n",
    "\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
